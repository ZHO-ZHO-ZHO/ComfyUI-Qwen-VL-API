import os
import io
import json
import requests
import torch
import dashscope
from dashscope import MultiModalConversation
from io import BytesIO
from PIL import Image,  ImageChops
from datetime import datetime
import tempfile
import random
import platform
import hashlib

p = os.path.dirname(os.path.realpath(__file__))

def get_qwenvl_api_key():
    try:
        config_path = os.path.join(p, 'config.json')
        with open(config_path, 'r') as f:  
            config = json.load(f)
        api_key = config["QWENVL_API_KEY"]
    except:
        print("å‡ºé”™å•¦ Error: API key is required")
        return ""
    return api_key


class QWenVL_API_S_Zho:

    def __init__(self):
        self.api_key = get_qwenvl_api_key()
        if self.api_key is not None:
            dashscope.api_key=self.api_key

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",), 
                "prompt": ("STRING", {"default": "Describe this image", "multiline": True}),
                "model_name": (["qwen-vl-plus", "qwen-vl-max"],),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}), 
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("text",)
    FUNCTION = "qwen_vl_generation"

    CATEGORY = "Zhoæ¨¡å—ç»„/ğŸ’«QWenVL"

    def tensor_to_image(self, tensor):
        # ç¡®ä¿å¼ é‡æ˜¯åœ¨CPUä¸Š
        tensor = tensor.cpu()
    
        # å°†å¼ é‡æ•°æ®è½¬æ¢ä¸º0-255èŒƒå›´å¹¶è½¬æ¢ä¸ºæ•´æ•°
        # è¿™é‡Œå‡è®¾å¼ é‡å·²ç»æ˜¯H x W x Cæ ¼å¼
        image_np = tensor.squeeze().mul(255).clamp(0, 255).byte().numpy()
    
        # åˆ›å»ºPILå›¾åƒ
        image = Image.fromarray(image_np, mode='RGB')
        return image

    def qwen_vl_generation(self, image, prompt, model_name, seed):
        if not self.api_key:
            raise ValueError("API key is required")
        
        if image == None:
            raise ValueError("qwen_vl needs a image")
        else:
            # è½¬æ¢å›¾åƒ
            pil_image = self.tensor_to_image(image)

            # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶è·¯å¾„
            temp_directory = tempfile.gettempdir()
            unique_suffix = "_temp_" + ''.join(random.choice("abcdefghijklmnopqrstuvwxyz") for _ in range(5))
            filename = f"image{unique_suffix}.png"
            temp_image_path = os.path.join(temp_directory, filename)
            #temp_image_url = f"file://{temp_image_path}"

            # æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©æ­£ç¡®çš„æ–‡ä»¶URLæ ¼å¼
            if platform.system() == 'Windows':
                temp_image_url = f"file://{temp_image_path}"
            else:
                temp_image_url = f"file:///{temp_image_path}"

            temp_image_url = temp_image_url.replace('\\', '/')
            
            # ä¿å­˜å›¾åƒåˆ°ä¸´æ—¶è·¯å¾„
            pil_image.save(temp_image_path)


            messages = [
                {
                    "role": "user",
                    "content": [
                        {"image": temp_image_url},
                        {"text": prompt}
                    ]
                }
            ]

            #print("temp_image_url:", temp_image_url)
            #print("prompt:", prompt)

            torch.manual_seed(seed)

            response = dashscope.MultiModalConversation.call(model=model_name, messages=messages, seed=seed)
            #print(response)

            response_json = response
            if 'output' in response_json and 'choices' in response_json['output']:
                choices = response_json['output']['choices']
                if choices and 'message' in choices[0]:
                    message_content = choices[0]['message']['content']
                    if message_content and 'text' in message_content[0]:
                        text_output = message_content[0]['text']
                        #print(text_output)  
                    else:
                        print("No text content found.")
                else:
                    print("No message found in the first choice.")
            else:
                print("No choices found in the output.")

            os.remove(temp_image_path)
            #print("remove : done!" )
        
        return (text_output, )


class QWenVL_API_S_Multi_Zho:

    def __init__(self):
        self.api_key = get_qwenvl_api_key()
        self.messages = []  # åˆå§‹åŒ–å¯¹è¯å†å²ä¸ºç©º
        self.last_image_hash = None
        if self.api_key is not None:
            dashscope.api_key=self.api_key

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",), 
                "prompt": ("STRING", {"default": "Describe this image", "multiline": True}),
                "model_name": (["qwen-vl-plus", "qwen-vl-max"],),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}), 
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("text",)
    FUNCTION = "qwen_vl_generation"

    CATEGORY = "Zhoæ¨¡å—ç»„/ğŸ’«QWenVL"

    def tensor_to_image(self, tensor):
        # ç¡®ä¿å¼ é‡æ˜¯åœ¨CPUä¸Š
        tensor = tensor.cpu()
    
        # å°†å¼ é‡æ•°æ®è½¬æ¢ä¸º0-255èŒƒå›´å¹¶è½¬æ¢ä¸ºæ•´æ•°
        # è¿™é‡Œå‡è®¾å¼ é‡å·²ç»æ˜¯H x W x Cæ ¼å¼
        image_np = tensor.squeeze().mul(255).clamp(0, 255).byte().numpy()
    
        # åˆ›å»ºPILå›¾åƒ
        image = Image.fromarray(image_np, mode='RGB')
        return image

    def format_qwchat_history(self):
        formatted_history = []
        for message in self.messages:
            role = message['role']
            contents = message['content']
            for content in contents:
                if 'text' in content:
                    text = content['text']
                    formatted_message = f"{role}: {text}"
                    formatted_history.append(formatted_message)
            formatted_history.append("-" * 40)  # æ·»åŠ åˆ†éš”çº¿
        return "\n".join(formatted_history)

    def get_image_hash(self, pil_image):
        # å°†å›¾åƒè½¬æ¢ä¸ºå­—èŠ‚
        image_bytes = pil_image.tobytes()
        # ä½¿ç”¨å“ˆå¸Œå‡½æ•°è®¡ç®—å“ˆå¸Œå€¼
        return hashlib.md5(image_bytes).hexdigest()

    def qwen_vl_generation(self, image, prompt, model_name, seed):
        if not self.api_key:
            raise ValueError("API key is required")
        
        if image == None:
            raise ValueError("qwen_vl needs a image")
        else:
            # è½¬æ¢å›¾åƒ
            pil_image = self.tensor_to_image(image)

            # åœ¨å½“å‰æ–‡ä»¶ç›®å½•ä¸‹åˆ›å»º "qw" æ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            qw_folder = os.path.join(p, "qw")
            os.makedirs(qw_folder, exist_ok=True)

            # è·å–å½“å‰å›¾åƒçš„å“ˆå¸Œå€¼
            current_image_hash = self.get_image_hash(pil_image)
        
            # æ„å»ºæ–‡ä»¶å
            local_image_filename = f"image_{current_image_hash}.png"
            local_image_path = os.path.join(qw_folder, local_image_filename)

            # æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©æ­£ç¡®çš„æ–‡ä»¶URLæ ¼å¼
            if platform.system() == 'Windows':
                local_image_url = f"file://{local_image_path}"
            else:
                local_image_url = f"file:///{local_image_path}"

            # ä¿è¯è·¯å¾„ä¸­çš„åæ–œæ è¢«æ›¿æ¢ä¸ºæ­£æ–œæ 
            local_image_url = local_image_url.replace('\\', '/')

            # å¦‚æœå½“å‰å›¾åƒä¸ä¸Šæ¬¡çš„ä¸åŒ
            if current_image_hash != self.last_image_hash:
                pil_image.save(local_image_path)
                # æ›´æ–°last_image_hash
                self.last_image_hash = current_image_hash
                print(f"Image saved as {local_image_filename}")
            else:
                print("Image not saved as it is identical to the last one.")

            self.messages.append({
                "role": "user",
                "content": [
                    {"image": local_image_url},
                    {"text": prompt}
                ]
            })

            #print("local_image_url:", local_image_url)
            #print("prompt:", prompt)

            torch.manual_seed(seed)

            response = dashscope.MultiModalConversation.call(model=model_name, messages=self.messages, seed=seed)
            #print(response)

            # æ›´æ–°å¯¹è¯å†å²
            if response and response.output and response.output.choices:
                choice = response.output.choices[0]
                if choice and choice.message:
                    self.messages.append({'role': choice.message.role, 'content': choice.message.content})

            response_json = response
            if 'output' in response_json and 'choices' in response_json['output']:
                choices = response_json['output']['choices']
                if choices and 'message' in choices[0]:
                    message_content = choices[0]['message']['content']
                    if message_content and 'text' in message_content[0]:
                        text_output = message_content[0]['text']
                        #print(text_output)  
                    else:
                        print("No text content found.")
                else:
                    print("No message found in the first choice.")
            else:
                print("No choices found in the output.")


            # è·å–æ ¼å¼åŒ–çš„å¯¹è¯å†å²
            chat_history = self.format_qwchat_history()
        
        return (chat_history, )


NODE_CLASS_MAPPINGS = {
    "QWenVL_API_S_Zho": QWenVL_API_S_Zho,
    "QWenVL_API_S_Multi_Zho": QWenVL_API_S_Multi_Zho,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "QWenVL_API_S_Zho": "ãŠ™ï¸QWenVL_Zho",
    "QWenVL_API_S_Multi_Zho": "ãŠ™ï¸QWenVL_Chat_Zho",
}
